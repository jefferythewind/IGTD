{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, concatenate, Conv2D, BatchNormalization, ReLU, MaxPooling2D, \\\n",
    "    Flatten, AlphaDropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, roc_auc_score, accuracy_score, \\\n",
    "    matthews_corrcoef\n",
    "\n",
    "import configparser\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "def ID_mapping(l1, l2):\n",
    "    pos = {}\n",
    "    for i in range(len(l1)):\n",
    "        pos[l1[i]] = i\n",
    "    idd = np.array([pos[i] for i in l2])\n",
    "    return idd\n",
    "\n",
    "\n",
    "\n",
    "def load_example_data():\n",
    "    res = pd.read_csv('../Data/Example_Drug_Response_Data.txt', sep='\\t', engine='c',\n",
    "                      na_values=['na', '-', ''], header=0, index_col=None)\n",
    "\n",
    "    files = os.listdir('../Data/Example_Drug_Descriptor_Image_Data/')\n",
    "    image = np.empty((len(files), 50, 50, 1))\n",
    "    sample = []\n",
    "    id = []\n",
    "    for i in range(len(files)):\n",
    "        if files[i].split('.')[1] == 'txt' and files[i].split('_')[0] == 'Drug':\n",
    "            id.append(i)\n",
    "            data = pd.read_csv('../Data/Example_Drug_Descriptor_Image_Data/' + files[i], sep='\\t', engine='c',\n",
    "                               na_values=['na', '-', ''], header=None, index_col=None)\n",
    "            image[i, :, :, 0] = data.values\n",
    "            sample.append(files[i].split('.txt')[0])\n",
    "    image = image[id, :, :, :]\n",
    "    drug = {}\n",
    "    drug['data'] = image\n",
    "    drug['sample'] = sample\n",
    "\n",
    "    files = os.listdir('../Data/Example_Gene_Expression_Image_Data/')\n",
    "    image = np.empty((len(files), 50, 50, 1))\n",
    "    sample = []\n",
    "    id = []\n",
    "    for i in range(len(files)):\n",
    "        if files[i].split('.')[1] == 'txt' and files[i].split('_')[0] == 'CCL':\n",
    "            id.append(i)\n",
    "            data = pd.read_csv('../Data/Example_Gene_Expression_Image_Data/' + files[i], sep='\\t', engine='c',\n",
    "                               na_values=['na', '-', ''], header=None, index_col=None)\n",
    "            image[i, :, :, 0] = data.values\n",
    "            sample.append(files[i].split('.txt')[0])\n",
    "    image = image[id, :, :, :]\n",
    "    ccl = {}\n",
    "    ccl['data'] = image\n",
    "    ccl['sample'] = sample\n",
    "\n",
    "    return res, ccl, drug\n",
    "\n",
    "\n",
    "\n",
    "def get_data_for_cross_validation(res, ccl, drug, sampleID):\n",
    "\n",
    "    trainData = []\n",
    "    valData = []\n",
    "    testData = []\n",
    "\n",
    "    train_idd = ID_mapping(drug['sample'], res.iloc[sampleID['trainID'], :].Drug)\n",
    "    trainData.append(drug['data'][train_idd, :, :, :])\n",
    "    val_idd = ID_mapping(drug['sample'], res.iloc[sampleID['valID'], :].Drug)\n",
    "    valData.append(drug['data'][val_idd, :, :, :])\n",
    "    test_idd = ID_mapping(drug['sample'], res.iloc[sampleID['testID'], :].Drug)\n",
    "    testData.append(drug['data'][test_idd, :, :, :])\n",
    "\n",
    "    train_idd = ID_mapping(ccl['sample'], res.iloc[sampleID['trainID'], :].CCL)\n",
    "    trainData.append(ccl['data'][train_idd, :, :, :])\n",
    "    val_idd = ID_mapping(ccl['sample'], res.iloc[sampleID['valID'], :].CCL)\n",
    "    valData.append(ccl['data'][val_idd, :, :, :])\n",
    "    test_idd = ID_mapping(ccl['sample'], res.iloc[sampleID['testID'], :].CCL)\n",
    "    testData.append(ccl['data'][test_idd, :, :, :])\n",
    "\n",
    "    trainLabel = res.iloc[sampleID['trainID'], :].AUC.values\n",
    "    valLabel = res.iloc[sampleID['valID'], :].AUC.values\n",
    "    testLabel = res.iloc[sampleID['testID'], :].AUC.values\n",
    "\n",
    "    train = {}\n",
    "    train['data'] = trainData\n",
    "    train['label'] = trainLabel\n",
    "    train['sample'] = res.iloc[sampleID['trainID'], :].CCL + '|' + res.iloc[sampleID['trainID'], :].Drug\n",
    "    val = {}\n",
    "    val['data'] = valData\n",
    "    val['label'] = valLabel\n",
    "    val['sample'] = res.iloc[sampleID['valID'], :].CCL + '|' + res.iloc[sampleID['valID'], :].Drug\n",
    "    test = {}\n",
    "    test['data'] = testData\n",
    "    test['label'] = testLabel\n",
    "    test['sample'] = res.iloc[sampleID['testID'], :].CCL + '|' + res.iloc[sampleID['testID'], :].Drug\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "\n",
    "def get_model_parameter(model_file):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(model_file)\n",
    "    section = config.sections()\n",
    "    params = {}\n",
    "    for sec in section:\n",
    "        for k, v in config.items(sec):\n",
    "            if k not in params:\n",
    "                params[k] = eval(v)\n",
    "    return params\n",
    "\n",
    "\n",
    "\n",
    "def get_DNN_optimizer(opt_name):\n",
    "    if opt_name == 'SGD':\n",
    "        optimizer = optimizers.GSD()\n",
    "    elif opt_name == 'SGD_momentum':\n",
    "        optimizer = optimizers.GSD(momentum=0.9)\n",
    "    elif opt_name == 'SGD_momentum_nesterov':\n",
    "        optimizer = optimizers.GSD(momentum=0.9, nesterov=True)\n",
    "    elif opt_name == 'RMSprop':\n",
    "        optimizer = optimizers.RMSprop()\n",
    "    elif opt_name == 'Adagrad':\n",
    "        optimizer = optimizers.Adagrad()\n",
    "    elif opt_name == 'Adadelta':\n",
    "        optimizer = optimizers.Adadelta()\n",
    "    elif opt_name == 'Adam':\n",
    "        optimizer = optimizers.Adam()\n",
    "    elif opt_name == 'Adam_amsgrad':\n",
    "        optimizer = optimizers.Adam(amsgrad=True)\n",
    "    else:\n",
    "        optimizer = optimizers.Adam()\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "\n",
    "def calculate_batch_size(num_sample, paraDNN):\n",
    "    # max_half_num_batch: the number of batches will not be larger than 2 * max_half_num_batch\n",
    "    max_half_num_batch = paraDNN['max_half_num_batch']\n",
    "    if num_sample < max_half_num_batch * 4:\n",
    "        batch_size = 2\n",
    "    elif num_sample < max_half_num_batch * 8:\n",
    "        batch_size = 4\n",
    "    elif num_sample < max_half_num_batch * 16:\n",
    "        batch_size = 8\n",
    "    elif num_sample < max_half_num_batch * 32:\n",
    "        batch_size = 16\n",
    "    elif num_sample < max_half_num_batch * 64:\n",
    "        batch_size = 32\n",
    "    elif num_sample < max_half_num_batch * 128:\n",
    "        batch_size = 64\n",
    "    elif num_sample < max_half_num_batch * 256:\n",
    "        batch_size = 128\n",
    "    else:\n",
    "        batch_size = 256\n",
    "\n",
    "    return batch_size\n",
    "\n",
    "\n",
    "\n",
    "class CNN2D_Regressor():\n",
    "    # This is the class for 2-dimensional convolutional neural network regressors (CNN2D_Regressor).\n",
    "    # The model can accept more than one sizes of filter, such [3, 5].\n",
    "    def __init__(self, params, input_data_dim, dropout):\n",
    "        # params: dict, CNN2D model parameters\n",
    "        # input_data_dim: a list. Each element of the list includes two positive integers, the dimension of input images\n",
    "        # dropout: dropout rate, all layers use the same dropout rate\n",
    "\n",
    "        self.params = params\n",
    "        self.dropout = dropout\n",
    "        self.input_data_dim = input_data_dim\n",
    "\n",
    "        num_kernel_size = len(self.params['kernel_size'])\n",
    "        num_conv_layer = []\n",
    "        for i in range(num_kernel_size):\n",
    "            num_conv_layer.append(len(self.params['num_kernel'][i]))\n",
    "        num_dense_layer = len(self.params['network_layers'])\n",
    "\n",
    "        input = []\n",
    "        input2List = []\n",
    "        num_input = len(self.input_data_dim)\n",
    "        for input_id in range(num_input):\n",
    "            in_id = Input(shape=(self.input_data_dim[input_id][0], self.input_data_dim[input_id][1], 1),\n",
    "                          name='Input_' + str(input_id))\n",
    "            input.append(in_id)\n",
    "            for j in range(num_kernel_size):\n",
    "                min_row_size = self.params['pool_size'][j][0] * 2 + self.params['kernel_size'][j][0] - 1\n",
    "                min_col_size = self.params['pool_size'][j][1] * 2 + self.params['kernel_size'][j][1] - 1\n",
    "                for i in range(num_conv_layer[j]):\n",
    "                    if i == 0:\n",
    "                        d = Conv2D(filters=self.params['num_kernel'][j][i], kernel_size=self.params['kernel_size'][j],\n",
    "                                   strides=self.params['strides'][j], padding='valid', data_format='channels_last',\n",
    "                                   name='Conv2D_' + str(i) + '_Kernel_' + str(j) + '_Input_' + str(input_id))(in_id)\n",
    "                    else:\n",
    "                        d = Conv2D(filters=self.params['num_kernel'][j][i], kernel_size=self.params['kernel_size'][j],\n",
    "                                   strides=self.params['strides'][j], padding='valid', data_format='channels_last',\n",
    "                                   name='Conv2D_' + str(i) + '_Kernel_' + str(j) + '_Input_' + str(input_id))(d)\n",
    "                    d = BatchNormalization(axis=-1, name='BatchNorm_' + str(i) + '_Kernel_' + str(j) + '_Input_'\n",
    "                                                         + str(input_id))(inputs=d)\n",
    "                    if self.params['subnetwork_activation'] == 'relu':\n",
    "                        d = ReLU(name='ReLU_' + str(i) + '_Kernel_' + str(j) + '_Input_' + str(input_id))(d)\n",
    "                    else:\n",
    "                        raise TypeError(\"Activation is not ReLU in subnetwork.\")\n",
    "                    d = MaxPooling2D(pool_size=self.params['pool_size'][j], name='MaxPooling_' + str(i) + '_Kernel_'\n",
    "                        + str(j) + '_Input_' + str(input_id))(d)\n",
    "                    dim = np.array(d.shape.as_list())\n",
    "                    flag_0 = dim[1] < min_row_size\n",
    "                    flag_1 = dim[2] < min_col_size\n",
    "                    if flag_0 or flag_1:\n",
    "                        break\n",
    "                d = Flatten()(d)\n",
    "                input2List.append(d)\n",
    "\n",
    "        if num_input > 1:\n",
    "            d = concatenate(input2List, name='concatenation')\n",
    "        for i in range(num_dense_layer):\n",
    "            if self.params['activation'] == 'selu':\n",
    "                d = Dense(self.params['network_layers'][i], activation=self.params['activation'], name='Dense_' + str(i),\n",
    "                          kernel_initializer='lecun_normal')(d)\n",
    "            else:\n",
    "                d = Dense(self.params['network_layers'][i], activation=self.params['activation'], name='Dense_' + str(i))(d)\n",
    "            if i != num_dense_layer - 1:\n",
    "                if self.params['activation'] == 'selu':\n",
    "                    d = AlphaDropout(self.dropout, name='Dropout_Dense_' + str(i))(d)\n",
    "                else:\n",
    "                    d = Dropout(self.dropout, name='Dropout_Dense_' + str(i))(d)\n",
    "\n",
    "        output = Dense(1, name='output')(d)\n",
    "        if num_input > 1:\n",
    "            model = Model(inputs=input, outputs=output)\n",
    "        else:\n",
    "            model = Model(inputs=input[0], outputs=output)\n",
    "        model.compile(optimizer=get_DNN_optimizer(self.params['optimizer']), loss=self.params['loss'])\n",
    "        print(model.summary())\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "\n",
    "class CNN2D_Classifier():\n",
    "    # This is the class for 2-dimensional convolutional neural network regressors (CNN2D_Regressor).\n",
    "    # The model can accept more than one sizes of filter, such [3, 5].\n",
    "    def __init__(self, params, input_data_dim, num_class, dropout):\n",
    "        # params: dict, CNN2D model parameters\n",
    "        # input_data_dim: a list. Each element of the list includes two positive integers, the dimension of input images\n",
    "        # dropout: dropout rate, all layers use the same dropout rate\n",
    "\n",
    "        self.params = params\n",
    "        self.dropout = dropout\n",
    "        self.num_class = num_class\n",
    "        self.input_data_dim = input_data_dim\n",
    "\n",
    "        num_kernel_size = len(self.params['kernel_size'])\n",
    "        num_conv_layer = []\n",
    "        for i in range(num_kernel_size):\n",
    "            num_conv_layer.append(len(self.params['num_kernel'][i]))\n",
    "        num_dense_layer = len(self.params['network_layers'])\n",
    "\n",
    "        input = []\n",
    "        input2List = []\n",
    "        num_input = len(self.input_data_dim)\n",
    "        for input_id in range(num_input):\n",
    "            in_id = Input(shape=(self.input_data_dim[input_id][0], self.input_data_dim[input_id][1], 1),\n",
    "                          name='Input_' + str(input_id))\n",
    "            input.append(in_id)\n",
    "            for j in range(num_kernel_size):\n",
    "                min_row_size = self.params['pool_size'][j][0] * 2 + self.params['kernel_size'][j][0] - 1\n",
    "                min_col_size = self.params['pool_size'][j][1] * 2 + self.params['kernel_size'][j][1] - 1\n",
    "                for i in range(num_conv_layer[j]):\n",
    "                    if i == 0:\n",
    "                        d = Conv2D(filters=self.params['num_kernel'][j][i], kernel_size=self.params['kernel_size'][j],\n",
    "                                   strides=self.params['strides'][j], padding='valid', data_format='channels_last',\n",
    "                                   name='Conv2D_' + str(i) + '_Kernel_' + str(j) + '_Input_' + str(input_id))(in_id)\n",
    "                    else:\n",
    "                        d = Conv2D(filters=self.params['num_kernel'][j][i], kernel_size=self.params['kernel_size'][j],\n",
    "                                   strides=self.params['strides'][j], padding='valid', data_format='channels_last',\n",
    "                                   name='Conv2D_' + str(i) + '_Kernel_' + str(j) + '_Input_' + str(input_id))(d)\n",
    "                    d = BatchNormalization(axis=-1, name='BatchNorm_' + str(i) + '_Kernel_' + str(j) + '_Input_'\n",
    "                                                         + str(input_id))(inputs=d)\n",
    "                    if self.params['subnetwork_activation'] == 'relu':\n",
    "                        d = ReLU(name='ReLU_' + str(i) + '_Kernel_' + str(j) + '_Input_' + str(input_id))(d)\n",
    "                    else:\n",
    "                        raise TypeError(\"Activation is not ReLU in subnetwork.\")\n",
    "                    d = MaxPooling2D(pool_size=self.params['pool_size'][j], name='MaxPooling_' + str(i) + '_Kernel_'\n",
    "                        + str(j) + '_Input_' + str(input_id), padding='same')(d)\n",
    "                    dim = np.array(d.shape.as_list())\n",
    "                    flag_0 = dim[1] < min_row_size\n",
    "                    flag_1 = dim[2] < min_col_size\n",
    "                    if flag_0 or flag_1:\n",
    "                        break\n",
    "                d = Flatten()(d)\n",
    "                input2List.append(d)\n",
    "\n",
    "        if num_input > 1:\n",
    "            d = concatenate(input2List, name='concatenation')\n",
    "        for i in range(num_dense_layer):\n",
    "            if self.params['activation'] == 'selu':\n",
    "                d = Dense(self.params['network_layers'][i], activation=self.params['activation'], name='Dense_' + str(i),\n",
    "                          kernel_initializer='lecun_normal')(d)\n",
    "            else:\n",
    "                d = Dense(self.params['network_layers'][i], activation=self.params['activation'], name='Dense_' + str(i))(d)\n",
    "            if i != num_dense_layer - 1:\n",
    "                if self.params['activation'] == 'selu':\n",
    "                    d = AlphaDropout(self.dropout, name='Dropout_Dense_' + str(i))(d)\n",
    "                else:\n",
    "                    d = Dropout(self.dropout, name='Dropout_Dense_' + str(i))(d)\n",
    "\n",
    "        output = Dense(self.num_class, activation='softmax', name='output')(d)\n",
    "        if num_input > 1:\n",
    "            model = Model(inputs=input, outputs=output)\n",
    "        else:\n",
    "            model = Model(inputs=input[0], outputs=output)\n",
    "        model.compile(optimizer=get_DNN_optimizer(self.params['optimizer']), loss=self.params['loss'])\n",
    "        print(model.summary())\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "\n",
    "def CNN2D_Regression_Analysis(train, resultFolder, para, val=None, test=None):\n",
    "    '''\n",
    "    This function does CNN2D regression analysis without HPO.\n",
    "\n",
    "    Input:\n",
    "    train: a dictionary of three elements. data is an array of (sample, height, width).\n",
    "        label is a series of the prediction target. sample is an array of sample names.\n",
    "    val: a dictionary for validation data.\n",
    "    resultFolder: directory to save models, features, and results\n",
    "    para: parameters used for model training\n",
    "    test: a dictionary for testing data. Default is None.\n",
    "\n",
    "    Return:\n",
    "    predResult: a dictionary including three series, which are prediction results on the training, validation,\n",
    "        and testing sets.\n",
    "    perM: an array of training and validation losses with different dropout rates and epochs.\n",
    "    perf: a 3 by 7 data frame including the prediction performance on training, validation, and testing sets.\n",
    "    winningModel: a string giving the epoch number and dropout rate of the best model with the smallest validation loss.\n",
    "    '''\n",
    "\n",
    "    if os.path.exists(resultFolder):\n",
    "        shutil.rmtree(resultFolder)\n",
    "    os.mkdir(resultFolder)\n",
    "\n",
    "    trainData = train['data']\n",
    "    trainLabel = train['label']\n",
    "    trainSample = train['sample']\n",
    "\n",
    "    if isinstance(trainData, list):\n",
    "        batch_size = calculate_batch_size(trainData[0].shape[0], para)\n",
    "    else:\n",
    "        batch_size = calculate_batch_size(trainData.shape[0], para)\n",
    "\n",
    "    # batch_size = 5000\n",
    "    # print(batch_size)\n",
    "\n",
    "    if val is not None:\n",
    "        valData = val['data']\n",
    "        valLabel = val['label']\n",
    "        valSample = val['sample']\n",
    "    else:\n",
    "        valData = None\n",
    "        valLabel = None\n",
    "        valSample = None\n",
    "\n",
    "    if test is not None:\n",
    "        testData = test['data']\n",
    "        testSample = test['sample']\n",
    "        if test['label'] is not None:\n",
    "            testLabel = test['label']\n",
    "        else:\n",
    "            testLabel = None\n",
    "    else:\n",
    "        testData = None\n",
    "        testLabel = None\n",
    "        testSample = None\n",
    "\n",
    "    if isinstance(trainData, list):\n",
    "        input_data_dim = []\n",
    "        for i in range(len(trainData)):\n",
    "            input_data_dim.append([trainData[i].shape[1], trainData[i].shape[2]])\n",
    "    else:\n",
    "        input_data_dim = [[trainData.shape[1], trainData.shape[2]]]\n",
    "\n",
    "    perM = {}\n",
    "    for i in ['train', 'val']:\n",
    "        perM[i] = np.empty((len(para['drop']), para['epochs']))\n",
    "        perM[i].fill(np.inf)\n",
    "        perM[i] = pd.DataFrame(perM[i], index=['dropout_' + str(j) for j in para['drop']],\n",
    "            columns=['epoch_' + str(j) for j in range(para['epochs'])])\n",
    "\n",
    "    for dpID in range(len(para['drop'])):\n",
    "        label = 'dropout_' + str(para['drop'][dpID])\n",
    "        print(label)\n",
    "\n",
    "        if val is not None:\n",
    "            monitor = 'val_loss'\n",
    "        else:\n",
    "            monitor = 'loss'\n",
    "        train_logger = CSVLogger(resultFolder + '/log_dropout_' + str(para['drop'][dpID]) + '.csv')\n",
    "        model_saver = ModelCheckpoint(resultFolder + '/model_dropout_' + str(para['drop'][dpID]) + '.h5',\n",
    "                                      monitor=monitor, save_best_only=True, save_weights_only=False)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor=monitor, factor=para['rlr_factor'], patience=para['rlr_patience'],\n",
    "                                      verbose=1, mode='auto', min_delta=para['rlr_min_delta'],\n",
    "                                      cooldown=para['rlr_cooldown'], min_lr=para['rlr_min_lr'])\n",
    "        early_stop = EarlyStopping(monitor=monitor, patience=para['es_patience'], min_delta=para['es_min_delta'],\n",
    "                                   verbose=1)\n",
    "        callbacks = [model_saver, train_logger, reduce_lr, early_stop]\n",
    "\n",
    "        temp = CNN2D_Regressor(para, input_data_dim, para['drop'][dpID])\n",
    "\n",
    "        if val is not None:\n",
    "            history = temp.model.fit(x=trainData, y=trainLabel, batch_size=batch_size, epochs=para['epochs'],\n",
    "                verbose=para['verbose'], callbacks=callbacks, validation_data=(valData, valLabel), shuffle=True)\n",
    "        else:\n",
    "            history = temp.model.fit(x=trainData, y=trainLabel, batch_size=batch_size, epochs=para['epochs'],\n",
    "                verbose=para['verbose'], callbacks=callbacks, validation_data=None, shuffle=True)\n",
    "        numEpoch = len(history.history['loss'])\n",
    "        i = np.where(perM['train'].index == label)[0]\n",
    "        perM['train'].iloc[i, :numEpoch] = history.history['loss']\n",
    "        if val is not None:\n",
    "            numEpoch = len(history.history['val_loss'])\n",
    "            i = np.where(perM['val'].index == label)[0]\n",
    "            perM['val'].iloc[i, :numEpoch] = history.history['val_loss']\n",
    "\n",
    "        backend.clear_session()\n",
    "\n",
    "    if val is not None:\n",
    "        dpID, epID = np.unravel_index(np.argmin(perM['val'].values, axis=None), perM['val'].shape)\n",
    "    else:\n",
    "        dpID, epID = np.unravel_index(np.argmin(perM['train'].values, axis=None), perM['train'].shape)\n",
    "    model = load_model(resultFolder + '/model_dropout_' + str(para['drop'][dpID]) + '.h5')\n",
    "\n",
    "    for i in range(len(para['drop'])):\n",
    "        if i == dpID:\n",
    "            continue\n",
    "        os.remove(resultFolder + '/model_dropout_' + str(para['drop'][i]) + '.h5')\n",
    "        os.remove(resultFolder + '/log_dropout_' + str(para['drop'][i]) + '.csv')\n",
    "\n",
    "    predResult = {}\n",
    "    if test is not None:\n",
    "        predResult['test'] = pd.DataFrame(model.predict(testData), index=testSample, columns=['prediction'])\n",
    "    predResult['train'] = pd.DataFrame(model.predict(trainData), index=trainSample, columns=['prediction'])\n",
    "    if val is not None:\n",
    "        predResult['val'] = pd.DataFrame(model.predict(valData), index=valSample, columns=['prediction'])\n",
    "\n",
    "    backend.clear_session()\n",
    "\n",
    "    perf = np.empty((3, 7))\n",
    "    perf.fill(np.nan)\n",
    "    perf = pd.DataFrame(perf, columns=['R2', 'MSE', 'MAE', 'pCor', 'pCorPvalue', 'sCor', 'sCorPvalue'],\n",
    "                        index=['train', 'val', 'test'])\n",
    "    for k in ['train', 'val', 'test']:\n",
    "        if (eval(k + 'Data') is None) or (eval(k + 'Label') is None):\n",
    "            continue\n",
    "        perf.loc[k, 'R2'] = r2_score(eval(k + 'Label'), predResult[k].values[:, 0])\n",
    "        perf.loc[k, 'MSE'] = mean_squared_error(eval(k + 'Label'), predResult[k].values[:, 0])\n",
    "        perf.loc[k, 'MAE'] = mean_absolute_error(eval(k + 'Label'), predResult[k].values[:, 0])\n",
    "        rho, pval = stats.pearsonr(eval(k + 'Label'), predResult[k].values[:, 0])\n",
    "        perf.loc[k, 'pCor'] = rho\n",
    "        perf.loc[k, 'pCorPvalue'] = pval\n",
    "        rho, pval = stats.spearmanr(eval(k + 'Label'), predResult[k].values[:, 0])\n",
    "        perf.loc[k, 'sCor'] = rho\n",
    "        perf.loc[k, 'sCorPvalue'] = pval\n",
    "\n",
    "    return predResult, perM, perf, 'dropout_' + str(para['drop'][dpID]) + '_epoch_' + str(epID + 1), batch_size\n",
    "\n",
    "\n",
    "\n",
    "def CNN2D_Classification_Analysis(train, num_class, resultFolder, para, class_weight=None, val=None, test=None):\n",
    "    '''\n",
    "    This function does CNN2D regression analysis without HPO.\n",
    "\n",
    "    Input:\n",
    "    train: a dictionary of three elements. data is an array of (sample, height, width).\n",
    "        label is a series of the prediction target. sample is an array of sample names.\n",
    "    val: a dictionary for validation data.\n",
    "    resultFolder: directory to save models, features, and results\n",
    "    para: parameters used for model training\n",
    "    test: a dictionary for testing data. Default is None.\n",
    "\n",
    "    Return:\n",
    "    predResult: a dictionary including three series, which are prediction results on the training, validation,\n",
    "        and testing sets.\n",
    "    perM: an array of training and validation losses with different dropout rates and epochs.\n",
    "    perf: a 3 by 7 data frame including the prediction performance on training, validation, and testing sets.\n",
    "    winningModel: a string giving the epoch number and dropout rate of the best model with the smallest validation loss.\n",
    "    '''\n",
    "\n",
    "    if os.path.exists(resultFolder):\n",
    "        shutil.rmtree(resultFolder)\n",
    "    os.mkdir(resultFolder)\n",
    "\n",
    "    trainData = train['data']\n",
    "    trainLabel = train['label']\n",
    "    trainSample = train['sample']\n",
    "\n",
    "    if isinstance(trainData, list):\n",
    "        batch_size = calculate_batch_size(trainData[0].shape[0], para)\n",
    "    else:\n",
    "        batch_size = calculate_batch_size(trainData.shape[0], para)\n",
    "    # batch_size = 5000\n",
    "    # print(batch_size)\n",
    "\n",
    "    if val is not None:\n",
    "        valData = val['data']\n",
    "        valLabel = val['label']\n",
    "        valSample = val['sample']\n",
    "    else:\n",
    "        valData = None\n",
    "        valLabel = None\n",
    "        valSample = None\n",
    "\n",
    "    if test is not None:\n",
    "        testData = test['data']\n",
    "        testSample = test['sample']\n",
    "        if test['label'] is not None:\n",
    "            testLabel = test['label']\n",
    "        else:\n",
    "            testLabel = None\n",
    "    else:\n",
    "        testData = None\n",
    "        testLabel = None\n",
    "        testSample = None\n",
    "\n",
    "\n",
    "    if isinstance(trainData, list):\n",
    "        input_data_dim = []\n",
    "        for i in range(len(trainData)):\n",
    "            input_data_dim.append([trainData[i].shape[1], trainData[i].shape[2]])\n",
    "    else:\n",
    "        input_data_dim = [[trainData.shape[1], trainData.shape[2]]]\n",
    "\n",
    "    perM = {}\n",
    "    for i in ['train', 'val']:\n",
    "        perM[i] = np.empty((len(para['drop']), para['epochs']))\n",
    "        perM[i].fill(np.inf)\n",
    "        perM[i] = pd.DataFrame(perM[i], index=['dropout_' + str(j) for j in para['drop']],\n",
    "            columns=['epoch_' + str(j) for j in range(para['epochs'])])\n",
    "\n",
    "    if class_weight == 'balanced':\n",
    "        weight = len(trainLabel) / (num_class * np.bincount(trainLabel))\n",
    "        class_weight = {}\n",
    "        for i in range(num_class):\n",
    "            class_weight[i] = weight[i]\n",
    "\n",
    "    for dpID in range(len(para['drop'])):\n",
    "        label = 'dropout_' + str(para['drop'][dpID])\n",
    "        print(label)\n",
    "\n",
    "        if val is not None:\n",
    "            monitor = 'val_loss'\n",
    "        else:\n",
    "            monitor = 'loss'\n",
    "        train_logger = CSVLogger(resultFolder + '/log_dropout_' + str(para['drop'][dpID]) + '.csv')\n",
    "        model_saver = ModelCheckpoint(resultFolder + '/model_dropout_' + str(para['drop'][dpID]) + '.h5',\n",
    "                                      monitor=monitor, save_best_only=True, save_weights_only=False)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor=monitor, factor=para['rlr_factor'], patience=para['rlr_patience'],\n",
    "                                      verbose=1, mode='auto', min_delta=para['rlr_min_delta'],\n",
    "                                      cooldown=para['rlr_cooldown'], min_lr=para['rlr_min_lr'])\n",
    "        early_stop = EarlyStopping(monitor=monitor, patience=para['es_patience'], min_delta=para['es_min_delta'],\n",
    "                                   verbose=1)\n",
    "        callbacks = [model_saver, train_logger, reduce_lr, early_stop]\n",
    "\n",
    "        temp = CNN2D_Classifier(para, input_data_dim, num_class, para['drop'][dpID])\n",
    "\n",
    "        if val is not None:\n",
    "            history = temp.model.fit(x=trainData, y=trainLabel, batch_size=batch_size, epochs=para['epochs'],\n",
    "                verbose=para['verbose'], callbacks=callbacks, validation_data=(valData, valLabel),\n",
    "                class_weight=class_weight, shuffle=True)\n",
    "        else:\n",
    "            history = temp.model.fit(x=trainData, y=trainLabel, batch_size=batch_size, epochs=para['epochs'],\n",
    "                verbose=para['verbose'], callbacks=callbacks, validation_data=None, class_weight=class_weight,\n",
    "                shuffle=True)\n",
    "        numEpoch = len(history.history['loss'])\n",
    "        i = np.where(perM['train'].index == label)[0]\n",
    "        perM['train'].iloc[i, :numEpoch] = history.history['loss']\n",
    "        if val is not None:\n",
    "            numEpoch = len(history.history['val_loss'])\n",
    "            i = np.where(perM['val'].index == label)[0]\n",
    "            perM['val'].iloc[i, :numEpoch] = history.history['val_loss']\n",
    "\n",
    "        backend.clear_session()\n",
    "\n",
    "    if val is not None:\n",
    "        dpID, epID = np.unravel_index(np.argmin(perM['val'].values, axis=None), perM['val'].shape)\n",
    "    else:\n",
    "        dpID, epID = np.unravel_index(np.argmin(perM['train'].values, axis=None), perM['train'].shape)\n",
    "    model = load_model(resultFolder + '/model_dropout_' + str(para['drop'][dpID]) + '.h5')\n",
    "\n",
    "    for i in range(len(para['drop'])):\n",
    "        if i == dpID:\n",
    "            continue\n",
    "        os.remove(resultFolder + '/model_dropout_' + str(para['drop'][i]) + '.h5')\n",
    "        os.remove(resultFolder + '/log_dropout_' + str(para['drop'][i]) + '.csv')\n",
    "\n",
    "    predResult = {}\n",
    "    if test is not None:\n",
    "        predResult['test'] = {}\n",
    "        predResult['test']['proba'] = pd.DataFrame(model.predict(testData), index=testSample,\n",
    "                                                   columns=['proba_' + str(i) for i in range(num_class)])\n",
    "        predResult['test']['label'] = pd.DataFrame(np.argmax(a=predResult['test']['proba'].values, axis=1),\n",
    "                                                   index=predResult['test']['proba'].index, columns=['prediction'])\n",
    "    predResult['train'] = {}\n",
    "    predResult['train']['proba'] = pd.DataFrame(model.predict(trainData), index=trainSample,\n",
    "                                               columns=['proba_' + str(i) for i in range(num_class)])\n",
    "    predResult['train']['label'] = pd.DataFrame(np.argmax(a=predResult['train']['proba'].values, axis=1),\n",
    "                                               index=predResult['train']['proba'].index, columns=['prediction'])\n",
    "    if val is not None:\n",
    "        predResult['val'] = {}\n",
    "        predResult['val']['proba'] = pd.DataFrame(model.predict(valData), index=valSample,\n",
    "                                                  columns=['proba_' + str(i) for i in range(num_class)])\n",
    "        predResult['val']['label'] = pd.DataFrame(np.argmax(a=predResult['val']['proba'].values, axis=1),\n",
    "                                                  index=predResult['val']['proba'].index, columns=['prediction'])\n",
    "\n",
    "    backend.clear_session()\n",
    "\n",
    "    perf = np.empty((3, 3))\n",
    "    perf.fill(np.nan)\n",
    "    perf = pd.DataFrame(perf, columns=['ACC', 'AUROC', 'MCC'], index=['train', 'val', 'test'])\n",
    "    for k in ['train', 'val', 'test']:\n",
    "        if (eval(k + 'Data') is None) or (eval(k + 'Label') is None):\n",
    "            continue\n",
    "        perf.loc[k, 'ACC'] = accuracy_score(eval(k + 'Label'), predResult[k]['label'].values[:, 0])\n",
    "        if num_class == 2:\n",
    "            perf.loc[k, 'AUROC'] = roc_auc_score(eval(k + 'Label'), predResult[k]['proba'].values[:, 1])\n",
    "        else:\n",
    "            perf.loc[k, 'AUROC'] = roc_auc_score(keras.utils.to_categorical(eval(k + 'Label')), predResult[k]['proba'].values,\n",
    "                                                 labels=range(num_class), multi_class='ovr')\n",
    "        perf.loc[k, 'MCC'] = matthews_corrcoef(eval(k + 'Label'), predResult[k]['label'].values[:, 0])\n",
    "\n",
    "    return predResult, perM, perf, 'dropout_' + str(para['drop'][dpID]) + '_epoch_' + str(epID + 1), batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'network_layers': [500, 250, 125, 60],\n",
       " 'activation': 'relu',\n",
       " 'loss': 'mse',\n",
       " 'optimizer': 'Adam',\n",
       " 'drop': [0, 0.5],\n",
       " 'epochs': 10,\n",
       " 'rlr_factor': 0.1,\n",
       " 'rlr_min_delta': 1e-06,\n",
       " 'rlr_cooldown': 0,\n",
       " 'rlr_min_lr': 1e-06,\n",
       " 'rlr_patience': 10,\n",
       " 'es_patience': 20,\n",
       " 'es_min_delta': 1e-06,\n",
       " 'max_half_num_batch': 150,\n",
       " 'verbose': 2,\n",
       " 'kernel_size': [[5, 5]],\n",
       " 'num_kernel': [[16, 32, 64]],\n",
       " 'strides': [(1, 1)],\n",
       " 'pool_size': [(2, 2)],\n",
       " 'subnetwork_activation': 'relu'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Get Default Config Params\n",
    "'''\n",
    "para = get_model_parameter('../Data/Example_Model_Parameters/FCNN_Regressor.txt')\n",
    "subnetwork_para = get_model_parameter('../Data/Example_Model_Parameters/CNN2D_SubNetwork.txt')\n",
    "para.update(subnetwork_para)\n",
    "\n",
    "para['kernel_size']  = [[5,5]]\n",
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "These files are generated in the Forum Post Notebook\n",
    "'''\n",
    "data = np.load('../Data/image_data.npy')\n",
    "targets = np.load('../Data/targets.npy')\n",
    "\n",
    "train_idx = np.load('train_idx.npy')\n",
    "test_idx = np.load('test_idx.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_0 (InputLayer)        [(None, 39, 39, 1)]       0         \n",
      "                                                                 \n",
      " Conv2D_0_Kernel_0_Input_0 (  (None, 35, 35, 16)       416       \n",
      " Conv2D)                                                         \n",
      "                                                                 \n",
      " BatchNorm_0_Kernel_0_Input_  (None, 35, 35, 16)       64        \n",
      " 0 (BatchNormalization)                                          \n",
      "                                                                 \n",
      " ReLU_0_Kernel_0_Input_0 (Re  (None, 35, 35, 16)       0         \n",
      " LU)                                                             \n",
      "                                                                 \n",
      " MaxPooling_0_Kernel_0_Input  (None, 17, 17, 16)       0         \n",
      " _0 (MaxPooling2D)                                               \n",
      "                                                                 \n",
      " Conv2D_1_Kernel_0_Input_0 (  (None, 13, 13, 32)       12832     \n",
      " Conv2D)                                                         \n",
      "                                                                 \n",
      " BatchNorm_1_Kernel_0_Input_  (None, 13, 13, 32)       128       \n",
      " 0 (BatchNormalization)                                          \n",
      "                                                                 \n",
      " ReLU_1_Kernel_0_Input_0 (Re  (None, 13, 13, 32)       0         \n",
      " LU)                                                             \n",
      "                                                                 \n",
      " MaxPooling_1_Kernel_0_Input  (None, 6, 6, 32)         0         \n",
      " _0 (MaxPooling2D)                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1152)              0         \n",
      "                                                                 \n",
      " Dense_0 (Dense)             (None, 500)               576500    \n",
      "                                                                 \n",
      " Dropout_Dense_0 (Dropout)   (None, 500)               0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 250)               125250    \n",
      "                                                                 \n",
      " Dropout_Dense_1 (Dropout)   (None, 250)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 125)               31375     \n",
      "                                                                 \n",
      " Dropout_Dense_2 (Dropout)   (None, 125)               0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 60)                7560      \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 754,186\n",
      "Trainable params: 754,090\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "3729/3729 [==============================] - 18s 4ms/step - loss: 0.0548 - val_loss: 0.0503 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "3729/3729 [==============================] - 15s 4ms/step - loss: 0.0502 - val_loss: 0.0500 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "3729/3729 [==============================] - 15s 4ms/step - loss: 0.0500 - val_loss: 0.0501 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "3729/3729 [==============================] - 15s 4ms/step - loss: 0.0500 - val_loss: 0.0500 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "3729/3729 [==============================] - 15s 4ms/step - loss: 0.0500 - val_loss: 0.0501 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "3729/3729 [==============================] - 15s 4ms/step - loss: 0.0500 - val_loss: 0.0500 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "3729/3729 [==============================] - 15s 4ms/step - loss: 0.0499 - val_loss: 0.0500 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "3729/3729 [==============================] - 15s 4ms/step - loss: 0.0499 - val_loss: 0.0501 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "3729/3729 [==============================] - 15s 4ms/step - loss: 0.0498 - val_loss: 0.0501 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "3729/3729 [==============================] - 15s 4ms/step - loss: 0.0497 - val_loss: 0.0502 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "dpID = 0\n",
    "monitor = 'val_loss'\n",
    "resultFolder = '../Results/NMR'\n",
    "\n",
    "if os.path.exists(resultFolder):\n",
    "    shutil.rmtree(resultFolder)\n",
    "os.mkdir(resultFolder)\n",
    "\n",
    "\n",
    "train_logger = CSVLogger(resultFolder + '/log_dropout_' + str(para['drop'][dpID]) + '.csv')\n",
    "model_saver = ModelCheckpoint(resultFolder + '/model_dropout_' + str(para['drop'][dpID]) + '.h5',\n",
    "                              monitor=monitor, save_best_only=True, save_weights_only=False)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=monitor, factor=para['rlr_factor'], patience=para['rlr_patience'],\n",
    "                              verbose=1, mode='auto', min_delta=para['rlr_min_delta'],\n",
    "                              cooldown=para['rlr_cooldown'], min_lr=para['rlr_min_lr'])\n",
    "early_stop = EarlyStopping(monitor=monitor, patience=para['es_patience'], min_delta=para['es_min_delta'],\n",
    "                           verbose=1)\n",
    "callbacks = [model_saver, train_logger, reduce_lr, early_stop]\n",
    "\n",
    "temp = CNN2D_Regressor(para, [data.shape[1:3]], para['drop'][dpID])\n",
    "\n",
    "history = temp.model.fit(\n",
    "    x=data[train_idx[0]], \n",
    "    y=targets[train_idx[0]], \n",
    "    batch_size=256, \n",
    "    epochs=para['epochs'],\n",
    "    verbose=1, \n",
    "    callbacks=callbacks, \n",
    "    validation_data=(data[test_idx[0]], targets[test_idx[0]]), \n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240000\r"
     ]
    }
   ],
   "source": [
    "\n",
    "train_preds = []\n",
    "test_preds = []\n",
    "chunksize=5000\n",
    "i=0\n",
    "while i < len(train_idx[0]):\n",
    "    print(i, end='\\r')\n",
    "    train_preds.append( temp.model( data[train_idx[0]][i:i+chunksize] ) )\n",
    "    i += chunksize\n",
    "\n",
    "i=0\n",
    "while i < len(test_idx[0]):\n",
    "    print(i, end='\\r')\n",
    "    test_preds.append( temp.model( data[test_idx[0]][i:i+chunksize] ) )\n",
    "    i += chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.10003719],\n",
       "       [0.10003719, 1.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Train Corr'''\n",
    "np.corrcoef(np.hstack([np.vstack(train_preds), np.expand_dims( targets[train_idx[0]], -1 )]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.02249818],\n",
       "       [0.02249818, 1.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Test Corr'''\n",
    "np.corrcoef(np.hstack([np.vstack(test_preds), np.expand_dims( targets[test_idx[0]], -1 )]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Also generated from Forum Post Notebook'''\n",
    "baseline_preds = np.load('baseline_preds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.52317709],\n",
       "       [0.52317709, 1.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Correlation with Baseline Preds'''\n",
    "np.corrcoef(np.hstack([np.vstack(test_preds), np.expand_dims( baseline_preds, -1 )]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.03313168],\n",
       "       [0.03313168, 1.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''FYI Baseline Corr'''\n",
    "np.corrcoef(np.hstack([np.expand_dims( targets[test_idx[0]], -1 ), np.expand_dims( baseline_preds, -1 )]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
